import java.io.Serializable;
import java.sql.Date;
import java.text.DateFormat;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Comparator;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicLong;
import java.time.*;

import org.apache.commons.net.ntp.TimeStamp;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.dstream.DStream;
import org.json.simple.JSONArray;
import org.json.simple.JSONObject;
import org.json.simple.parser.*;

import com.google.gson.Gson;
import com.sun.jmx.snmp.Timestamp;

import scala.Function;
//import SparkstreamRSI.JsonStock;
//import SparkStream.StockRDD;
import scala.Tuple2;



public class SparkstreamRSI {
	
		
	
	//public static ArrayList<StockRDD> oldlist = new ArrayList<StockRDD>();
	
	public class JsonStock implements Comparable<JsonStock>,Serializable{
		private String symbol;
		private String timestamp;
		//private TimeStamp timestamp;
		private pricedata priceData;
		@Override
	    public String toString() {
	        return symbol + " - " + timestamp + " (" + priceData + ")";
	    }
		
		@Override
	    public int compareTo(JsonStock jsonstock) {

			SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
			java.util.Date parsedTimeStamp = null,jsonTimeStamp = null;
			try {
				parsedTimeStamp = dateFormat.parse(this.timestamp);
			} catch (ParseException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			try {
				jsonTimeStamp = dateFormat.parse(jsonstock.timestamp);
			} catch (ParseException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			
			//return this.timestamp.compareTo(jsonstock.timestamp);	
			return parsedTimeStamp.compareTo(jsonTimeStamp);
		}
	}
	
	public class pricedata implements Serializable{
		private Float open;
		private Float high;
		private Float low;
		private Float close;
		private Integer volume;
		@Override
	    public String toString() {
	        return open + " - " + high + " - " +low + " - " + close  + " - " + volume;
	    }
	}
	
	
		
	public static void main(String[] args) throws InterruptedException {
		
		

		SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("FirstSparkApplication")
				.set("spark.driver.allowMultipleContexts", "true");
		JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(60));
		
		
		Logger.getRootLogger().setLevel(Level.ERROR);

		JavaDStream<String> newlines = jssc.textFileStream("/Users/ravig/Documents/programs/stock_data");
		JavaDStream<String> newDstream=newlines.window(Durations.seconds(600),Durations.seconds(300));
		//newDstream.repartition(0);
		//JavaDStream<String> newDstream = newlines.window(Durations.seconds(120), Durations.seconds(60));
		
		SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
		java.util.Date parsedTimeStamp = null;
				
		newDstream.print();
		
				
		
		
		//DStream<Tuple2<String, Float>> Close_Dstream = newDstream.flatMap(new FlatMapFunction<String, JsonStock>() {
		//DStream<Tuple2<String,Tuple2<Float,Integer>>> 
		ArrayList<JsonStock> list = new ArrayList<JsonStock>();
		JavaDStream<JsonStock> Close_Dstream = newDstream.flatMap(new FlatMapFunction<String, JsonStock>() {
			private static final long serialVersionUID = 1L;
			
				
			public Iterator<JsonStock> call(String x) throws Exception {
				JSONParser jsonParser = new JSONParser();
				
				Gson gson = new Gson();

				try {
					Object obj = jsonParser.parse(x);
					JSONArray jsonstockcontent = (JSONArray) obj;
					
					for (Object obj1:jsonstockcontent) {
					JsonStock convertstock = gson.fromJson(obj1.toString(), JsonStock.class);
					list.add(convertstock);
					}	
				} catch (Exception e) {
					e.printStackTrace();
				}
				
				//System.out.println("Size of List is " +list.size());
				//				System.out.println(list.toString());
				return list.iterator();
			}
			
		});
		System.out.println(Close_Dstream.toString());
		JavaDStream<Long> count_stream = Close_Dstream.count(); 
		System.out.print("Number of RDD in this stream = ");
		count_stream.print();
		
		//DStream<Tuple2<String, Iterable<Float>>> mapstream 
		
		DStream<Tuple2<String, Float>> mapstream= Close_Dstream.mapToPair((x)->new Tuple2<String, JsonStock>(x.symbol,x))
				.mapToPair(new PairFunction<Tuple2<String, JsonStock>, JsonStock, String>() {
				
					private static final long serialVersionUID = 1L;


					@Override
					public Tuple2<SparkstreamRSI.JsonStock,String> call(Tuple2< String,SparkstreamRSI.JsonStock> item)
							throws Exception {
						// TODO Auto-generated method stub
						return item.swap();
					}
				}).transformToPair(x -> x.sortByKey())
				.mapToPair(new PairFunction<Tuple2<JsonStock, String >,  String,JsonStock>() {
					
					private static final long serialVersionUID = 1L;


					@Override
					public Tuple2<String, SparkstreamRSI.JsonStock> call(Tuple2< SparkstreamRSI.JsonStock, String> item)
							throws Exception {
						// TODO Auto-generated method stub
						return item.swap();
					}
				}).mapToPair((x) -> new Tuple2<String, Float>(x._1,x._2.priceData.close)).groupByKey().mapToPair(getRSI).
				dstream();
	
		mapstream.print();
	
		
		
		
				
			/*	map(list.sort(new Comparator<JsonStock>() {
			public int compare(JsonStock o1, JsonStock o2) {
				try {
					return dateFormat.parse(o1.timestamp).compareTo(dateFormat.parse(o2.timestamp));
				} catch (ParseException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				return 0;
			}
		})).mapToPair(x -> new Tuple2<String, Float>(x.symbol, x.priceData.close)).dstream();
				.mapValues(value -> new Tuple2<Float, Integer>(value, 1)).dstream();
				
				.reduceByKey((tuple1, tuple2) -> new <String, Tuple2<Float, Integer>>Tuple2<Float, Integer>(
						tuple1._1 + tuple2._1, tuple1._2 + tuple2._2))
			.mapToPair(getAverageByKey).mapToPair(new PairFunction<Tuple2<String, Float>, Float, String>() {
					private static final long serialVersionUID = 1L;

					@Override
					public Tuple2<Float, String> call(Tuple2<String, Float> item) throws Exception {
						return item.swap();
					}
				}).transformToPair(x -> x.sortByKey(false))
				
				.mapToPair(new PairFunction<Tuple2<Float, String>, String, Float>() {
					private static final long serialVersionUID = 1L;

					public Tuple2<String, Float> call(Tuple2<Float, String> item) throws Exception {
						return item.swap();
					}
				
				}).dstream();*/
		
		
		
		//Close_Dstream.print();
			
		
		jssc.start();
		jssc.awaitTermination();
		jssc.close();
		
	}/* End of Main */
	
	

	private static PairFunction<Tuple2<String, Tuple2<Float, Integer>>, String, Float> getAverageByKey = (tuple) -> {
		Tuple2<Float, Integer> val = tuple._2;
		Float total = val._1;
		Integer count = val._2;
		Tuple2<String, Float> averagePair = new Tuple2<String, Float>(tuple._1, total / count);
		return averagePair;
	};
	
	public int sortFunction(JsonStock x, JsonStock y) {
		SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss");
		try {
			return dateFormat.parse(x.timestamp).compareTo(dateFormat.parse(y.timestamp));
		} catch (ParseException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		return 0;
		
	}
	
	private static PairFunction<Tuple2<String,Iterable<Float>>, String, Float> getRSI = (tuple) -> {
		String Symbol = tuple._1;
		Iterable <Float> close_values = tuple._2;
		List<Float> data = new ArrayList<>();
		for(Float i: close_values) {
			data.add(i);
		}
		 
		
		int lastBar = data.size() - 1;
		int periodLength = lastBar;
		int firstBar = lastBar - periodLength + 1;
		if (firstBar < 0) {
			String msg = "Quote history length " + data.size() + " is insufficient to calculate the indicator.";
			throw new Exception(msg);
		}

		double aveGain = 0, aveLoss = 0;
		for (int bar = firstBar + 1; bar <= lastBar; bar++) {
			double change = data.get(bar) - data.get(bar - 1);
			if (change >= 0) {
				aveGain += change;
			} else {
				aveLoss += change;
			}
		}

		Float rs = (float) (aveGain / Math.abs(aveLoss));
		Float rsi = 100 - 100 / (1 + rs);
		
		Tuple2<String,Float> RSI = new Tuple2<String, Float>(Symbol,rsi);
		return RSI;
		
	};
	
	
		
	/*private static map_Function< <Integer>, JsonStock, JsonStock> Sort_list = {
			return dateFormat.parse(o1.timestamp).compareTo(dateFormat.parse(o2.timestamp));
	}*/
	 
} /* End of Spark Stream RSI class */
